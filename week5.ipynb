{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "165166dd",
   "metadata": {},
   "source": [
    "# DS Automation Assignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c195af74",
   "metadata": {},
   "source": [
    "Using our prepared churn data from week 2:\n",
    "- use TPOT to find an ML algorithm that performs best on the data\n",
    "    - Choose a metric you think is best to use for finding the best model; by default, it is accuracy but it could be AUC, precision, recall, etc. The week 3 FTE has some information on these different metrics.\n",
    "    - REMEMBER: TPOT only finds the optimized processing pipeline and model. It doesn't create the model. \n",
    "        - You can use `tpot.export('my_model_name.py')` (assuming you called your TPOT object tpot) and it will save a Python template with an example of the optimized pipeline. \n",
    "        - Use the template code saved from the `export()` function in your program.\n",
    "- create a Python script/file/module using code from the exported template above that\n",
    "    - create a function that takes a pandas dataframe as an input and returns the probability of churn for each row in the dataframe\n",
    "    - your Python file/function should print out the predictions for new data (new_churn_data.csv)\n",
    "    - the true values for the new data are [1, 0, 0, 1, 0] if you're interested\n",
    "- test your Python module and function with the new data, new_churn_data.csv\n",
    "- write a short summary of the process and results at the end of this notebook\n",
    "- upload this Jupyter Notebook and Python file to a Github repository, and turn in a link to the repository in the week 5 assignment dropbox\n",
    "\n",
    "*Optional* challenges:\n",
    "- return the probability of churn for each new prediction, and the percentile where that prediction is in the distribution of probability predictions from the training dataset (e.g. a high probability of churn like 0.78 might be at the 90th percentile)\n",
    "- use other autoML packages, such as TPOT, H2O, MLBox, etc, and compare performance and features with pycaret\n",
    "- create a class in your Python module to hold the functions that you created\n",
    "- accept user input to specify a file using a tool such as Python's `input()` function, the `click` package for command-line arguments, or a GUI\n",
    "- Use the unmodified churn data (new_unmodified_churn_data.csv) in your Python script. This will require adding the same preprocessing steps from week 2 since this data is like the original unmodified dataset from week 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "476ed965",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "# import the usual packages\n",
    "from tpot import TPOTClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from tpot import TPOTRegressor\n",
    "# Any results you write to the current directory are saved as output.\n",
    "import timeit \n",
    "from sklearn.ensemble import ExtraTreesClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.pipeline import make_pipeline, make_union\n",
    "from sklearn.preprocessing import Normalizer\n",
    "from tpot.builtins import StackingEstimator\n",
    "from tpot.export_utils import set_param_recursive\n",
    "\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6e6ef52",
   "metadata": {},
   "source": [
    "# Loading Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "e19b3f05",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('../week2/prepped_churned_data.csv', index_col='customerID')\n",
    "\n",
    "#making adjustments to synch with course-provided test data later on\n",
    "df.rename(columns = {'TotalCharges_by_tenure':'charge_per_tenure'}, inplace = True)\n",
    "df.drop('TotalCharges_by_tenure_log', axis=1, inplace=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "90e606be",
   "metadata": {},
   "source": [
    "## Splitting data for training\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "id": "ef1b19ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = df.drop('Churn', axis=1)\n",
    "targets = df['Churn']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63454e0a",
   "metadata": {},
   "source": [
    "## training data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "aaae044c",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(features, targets, train_size=0.7, test_size=0.3, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97fa6c86",
   "metadata": {},
   "source": [
    "## Running TPOT to get the best model parameters to use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "id": "015c29bc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Optimization Progress:   0%|          | 0/300 [00:00<?, ?pipeline/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Generation 1 - Current best internal CV score: 0.8017089678510999\n",
      "\n",
      "Generation 2 - Current best internal CV score: 0.8017089678510999\n",
      "\n",
      "Generation 3 - Current best internal CV score: 0.8017089678510999\n",
      "\n",
      "Generation 4 - Current best internal CV score: 0.8017091741983411\n",
      "\n",
      "Generation 5 - Current best internal CV score: 0.8017091741983411\n",
      "\n",
      "Best pipeline: RandomForestClassifier(input_matrix, bootstrap=True, criterion=entropy, max_features=0.2, min_samples_leaf=8, min_samples_split=4, n_estimators=100)\n",
      "0.7853080568720379\n",
      "CPU times: total: 30.7 s\n",
      "Wall time: 4min 8s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "tpot = TPOTClassifier(generations=5, population_size=50, verbosity=2, n_jobs=-1, random_state=42)\n",
    "tpot.fit(X_train, y_train)\n",
    "print(tpot.score(X_test, y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "id": "07a3cf5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "tpot.export('tpotmodel.py')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8e75d328",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9a6765a5",
   "metadata": {},
   "source": [
    "## loading \"newdf\" which is the new churn data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "id": "74f38320",
   "metadata": {},
   "outputs": [],
   "source": [
    "newdf = pd.read_csv('new_churn_data.csv', index_col='customerID')\n",
    "#need to rename Churn column  to target to work in function\n",
    "df.rename(columns = {'Churn':'target'}, inplace = True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1efeb311",
   "metadata": {},
   "source": [
    "## TPOT Pipeline is a function that I tested the .py development in, using the tpotmodel.py file generated above.  I use the training data from previous weeks to fit the \"proba\" of the five datapoints provided this week."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "id": "84d38644",
   "metadata": {},
   "outputs": [],
   "source": [
    "def TPOT_Pipeline(trainingdata,testdata):\n",
    "    features = trainingdata.drop('Churn', axis=1)\n",
    "    targets = trainingdata['Churn']\n",
    "    \n",
    "    tpot_data = trainingdata.copy()\n",
    "    \n",
    "    training_features, testing_features, training_target, testing_target = \\\n",
    "            train_test_split(features, tpot_data['target'], random_state=42)\n",
    "\n",
    "    # Average CV score on the training set was: 0.8017091741983411\n",
    "    exported_pipeline = RandomForestClassifier(bootstrap=True, criterion=\"entropy\", max_features=.5, min_samples_leaf=8, min_samples_split=4, n_estimators=100)\n",
    "    # Fix random state in exported estimator\n",
    "    if hasattr(exported_pipeline, 'random_state'):\n",
    "        setattr(exported_pipeline, 'random_state', 3)\n",
    "\n",
    "    #This fits our training data\n",
    "    exported_pipeline.fit(training_features, training_target)\n",
    "    \n",
    "    #This fits our test data \n",
    "    testing_features = testdata\n",
    "    for row in range(len(testing_features)):\n",
    "        results = exported_pipeline.predict_proba(testing_features)\n",
    "        prob = (results[row][0]*100).round(decimals = 2)\n",
    "        print(f'There is a {prob} probability for customer {testing_features.index[row]} to Churn' )\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e0c7d58",
   "metadata": {},
   "source": [
    "### here it is:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 297,
   "id": "82b569cb",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "There is a 69.01 probability for customer 9305-CKSKC to Churn\n",
      "There is a 68.91 probability for customer 1452-KNGVK to Churn\n",
      "There is a 83.93 probability for customer 6723-OKKJM to Churn\n",
      "There is a 84.54 probability for customer 7832-POPKP to Churn\n",
      "There is a 59.38 probability for customer 6348-TACGU to Churn\n"
     ]
    }
   ],
   "source": [
    "TPOT_Pipeline(df, newdf)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375914fa",
   "metadata": {},
   "source": [
    "### following the assignment, I now attempted to make a .py file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 298,
   "id": "3e0683c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import predict_Churn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 307,
   "id": "9e4817f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predictions:\n",
      "There is a 69.01 probability for customer 9305-CKSKC to Churn\n",
      "There is a 68.91 probability for customer 1452-KNGVK to Churn\n",
      "There is a 83.93 probability for customer 6723-OKKJM to Churn\n",
      "There is a 84.54 probability for customer 7832-POPKP to Churn\n",
      "There is a 59.38 probability for customer 6348-TACGU to Churn\n"
     ]
    }
   ],
   "source": [
    "run predict_Churn"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b2c81a",
   "metadata": {},
   "source": [
    "# It worked!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c49db562",
   "metadata": {},
   "source": [
    "# Summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9533a1cd",
   "metadata": {},
   "source": [
    "In this lesson we studied TPOTClassifier to identify the best model to use against our dataset.  When we applied our TPOT solution onto test data we did not have an easy time of identifying the true positive cases.  Customer 1 and 4 of the 5 sample customers churned.  Customer 1, 930-CKSKC was only 69% probably to Churn, yet they churned anyway.  Customer 4, 7832-POPKP scored the highest probability to churn, and they did in fact churn.  \n",
    "\n",
    "40% of the customers in the sample churned, this aligns with 2 of the top 3 of our highest probability-to-churn customers churning.  For the variance in the data, I think this is a pretty good model and can be very useful in determining the top quartile or so of customers to solicit for better deals to stay with our service.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9324fff1",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "vscode": {
   "interpreter": {
    "hash": "e5a5a5dcfe8c405964b888b7eb63d71c041385b923cf8ef6565c4fe595d89b61"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
